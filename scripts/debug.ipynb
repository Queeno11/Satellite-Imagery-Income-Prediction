{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import build_dataset\n",
    "import utils\n",
    "from importlib import reload\n",
    "\n",
    "# Load datasets\n",
    "# datasets, extents = build_dataset.load_satellite_datasets()\n",
    "# icpag = build_dataset.load_icpag_dataset()\n",
    "# icpag = build_dataset.assign_links_to_datasets(icpag, extents, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\"D:\\Maestría\\Tesis\\Repo\\data\\data_out\\test_datasets\\test_size128_tiles1_stacked1\\test_020010205.npy\"\n",
    "train = np.load(r\"/mnt/d/Maestría/Tesis/Repo/data/data_out/test_datasets/test_size128_tiles1_stacked1/test_020010205.npy\")\n",
    "pics = train.shape[0]\n",
    "fig, axs = plt.subplots(pics, 1, figsize=(6, 6*pics))\n",
    "for i in range(pics):\n",
    "    axs[i].imshow(train[i][:,:,:3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import Point\n",
    "point = Point(10, -20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import run_model\n",
    "reload(run_model)\n",
    "run_model.get_data(10, icpag, load=True, type=\"test\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = icpag.sample(1).link.values[0]\n",
    "ds = build_dataset.get_dataset_for_link(icpag, datasets, link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icpag[icpag.link == link].geometry.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import run_model\n",
    "run_model.create_train_test_dataframes(small_sample=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import build_dataset\n",
    "import geopandas as gpd\n",
    "sat_img_datasets, extents = build_dataset.load_satellite_datasets()\n",
    "# df_test = gpd.read_feather(rf\"{path_dataout}/test_datasets/test_dataframe.feather\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = gpd.read_feather(rf\"/mnt/d/Maestría/Tesis/Repo/data/data_out/test_datasets/test_dataframe.feather\")\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-03 21:06:39.736176: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-03 21:06:41.727514: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/nico/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/home/nico/miniconda3/envs/tf/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-12-03 21:06:44.186278: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-03 21:06:44.325561: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-03 21:06:44.326124: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "ERROR:tornado.application:Exception in callback functools.partial(<bound method OutStream._flush of <ipykernel.iostream.OutStream object at 0x7f2ceb99b640>>)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nico/miniconda3/envs/tf/lib/python3.9/site-packages/tornado/ioloop.py\", line 741, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/home/nico/miniconda3/envs/tf/lib/python3.9/site-packages/ipykernel/iostream.py\", line 604, in _flush\n",
      "    self.session.send(\n",
      "  File \"/home/nico/miniconda3/envs/tf/lib/python3.9/site-packages/jupyter_client/session.py\", line 862, in send\n",
      "    msg[\"tracker\"] = tracker\n",
      "UnboundLocalError: local variable 'tracker' referenced before assignment\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/d/Maestría/Tesis/Repo/scripts/debug.ipynb Cell 10\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Maestr%C3%ADa/Tesis/Repo/scripts/debug.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcompute_true_loss\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Maestr%C3%ADa/Tesis/Repo/scripts/debug.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mimportlib\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Maestr%C3%ADa/Tesis/Repo/scripts/debug.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m compute_true_loss\u001b[39m.\u001b[39;49mcompute_custom_loss_all_epochs(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Maestr%C3%ADa/Tesis/Repo/scripts/debug.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     models_dir\u001b[39m=\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/mnt/d/Maestría/Tesis/Repo/data/data_out/models_by_epoch/mobnet_v3_size128_tiles1_sample5\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Maestr%C3%ADa/Tesis/Repo/scripts/debug.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmobnet_v3_size128_tiles1_sample5\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Maestr%C3%ADa/Tesis/Repo/scripts/debug.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     tiles\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Maestr%C3%ADa/Tesis/Repo/scripts/debug.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Maestr%C3%ADa/Tesis/Repo/scripts/debug.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     resizing_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Maestr%C3%ADa/Tesis/Repo/scripts/debug.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     n_epochs\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Maestr%C3%ADa/Tesis/Repo/scripts/debug.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     n_bands\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Maestr%C3%ADa/Tesis/Repo/scripts/debug.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     stacked_images\u001b[39m=\u001b[39;49m[\u001b[39m1\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Maestr%C3%ADa/Tesis/Repo/scripts/debug.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Maestr%C3%ADa/Tesis/Repo/scripts/debug.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m )\n",
      "File \u001b[0;32m/mnt/d/Maestría/Tesis/Repo/scripts/compute_true_loss.py:515\u001b[0m, in \u001b[0;36mcompute_custom_loss_all_epochs\u001b[0;34m(models_dir, model_name, tiles, size, resizing_size, n_epochs, n_bands, stacked_images, generate, verbose)\u001b[0m\n\u001b[1;32m    512\u001b[0m q_images \u001b[39m=\u001b[39m link_images\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    514\u001b[0m \u001b[39m# Agrega al batch de valores reales / imagenes para la prediccion\u001b[39;00m\n\u001b[0;32m--> 515\u001b[0m images \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate([link_images, images], axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m    516\u001b[0m link_names \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([[link] \u001b[39m*\u001b[39m q_images, link_names], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    517\u001b[0m real_values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([[link_real_value] \u001b[39m*\u001b[39m q_images, real_values], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import compute_true_loss\n",
    "import importlib\n",
    "\n",
    "compute_true_loss.compute_custom_loss_all_epochs(\n",
    "    models_dir=r\"/mnt/d/Maestría/Tesis/Repo/data/data_out/models_by_epoch/mobnet_v3_size128_tiles1_sample5\",\n",
    "    model_name=\"mobnet_v3_size128_tiles1_sample5\",\n",
    "    tiles=1,\n",
    "    size=128,\n",
    "    resizing_size=128,\n",
    "    n_epochs=200,\n",
    "    n_bands=4,\n",
    "    stacked_images=[1],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder, links = compute_true_loss.generate_gridded_images(\n",
    "    tiles=1,\n",
    "    size=128,\n",
    "    resizing_size=128,\n",
    "    n_bands=4,\n",
    "    stacked_images=[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.loc[df_test[\"link\"]==20010201, \"geometry\"].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"link\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2]\n",
    "'_'.join(str(x) for x in a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ds\n",
    "test.where(test.band_data<100,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.isel(x=slice(0,100), y=slice(0,100)) - 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.band_data.isel(x=slice(0,10000), y=slice(0,10000)).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import skimage\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,6))\n",
    "\n",
    "clip_img_a = image[:3,:,:]\n",
    "clip_img_a = np.moveaxis(clip_img_a,0,2)\n",
    "clip_img_a = skimage.exposure.equalize_hist(clip_img_a)\n",
    "ax[0].imshow(clip_img_a)\n",
    "\n",
    "clip_img_b = image[4:7,:,:]\n",
    "clip_img_b = np.moveaxis(clip_img_b,0,2)\n",
    "clip_img_b = skimage.exposure.equalize_hist(clip_img_b)\n",
    "ax[1].imshow(clip_img_b)\n",
    "clip_img_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stretch(image, minimum, maximum):\n",
    "    image = (image - minimum) / (maximum - minimum)\n",
    "    image[image < 0] = 0\n",
    "    image[image > 1] = 1\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[ds<100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds - ds.band_data.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stretch(ds, ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(dataset, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import build_dataset\n",
    "\n",
    "census_data = build_dataset.load_icpag_dataset(variable=\"ln_pred_inc_mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import run_model\n",
    "\n",
    "df_train, df_test, sat_img_dataset = run_model.create_train_test_dataframes(\n",
    "        small_sample=False\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_data.to_parquet(\"census_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.712655037075141"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse_train</th>\n",
       "      <th>mse_test_img</th>\n",
       "      <th>mse_test_rc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.008470</td>\n",
       "      <td>0.793250</td>\n",
       "      <td>0.079780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.725519</td>\n",
       "      <td>0.815887</td>\n",
       "      <td>0.254955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.343850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.846584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.237404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.640422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.773620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.588329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.254390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mse_train  mse_test_img  mse_test_rc\n",
       "0    1.008470      0.793250     0.079780\n",
       "1    0.725519      0.815887     0.254955\n",
       "2         NaN           NaN     0.500739\n",
       "3         NaN           NaN     0.343850\n",
       "4         NaN           NaN     0.846584\n",
       "..        ...           ...          ...\n",
       "95        NaN           NaN     0.237404\n",
       "96        NaN           NaN     0.640422\n",
       "97        NaN           NaN     0.773620\n",
       "98        NaN           NaN     0.588329\n",
       "99        NaN           NaN     0.254390\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_dir=r\"/mnt/d/Maestría/Tesis/Repo/data/data_out/models_by_epoch/mobnet_v3_size128_tiles1_sample5\",\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "mse_test  = pd.DataFrame().from_dict(mse_epochs, orient=\"index\", columns=[\"mse_test_rc\"])\n",
    "mse_train = pd.read_csv(f\"/mnt/d/Maestría/Tesis/Repo/data/data_out/models_by_epoch/mobnet_v3_history.csv\")\\\n",
    "                [[\"mean_squared_error\", \"val_mean_squared_error\"]].rename(columns={\"mean_squared_error\":\"mse_train\",\"val_mean_squared_error\":\"mse_test_img\"})\n",
    "mse_train.join(mse_test, how=\"outer\").to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "import numpy as np\n",
    "\n",
    "# Extract data from TB\n",
    "experiment_id = \"RAqyLXG2RMKkAXMmNEDV7Q\"\n",
    "experiment = tb.data.experimental.ExperimentFromDev(experiment_id)\n",
    "df = experiment.get_scalars()\n",
    "\n",
    "for kind in [\"train\", \"validation\"]:\n",
    "    # Keep only train values\n",
    "    base = df[df.run.str.contains(f'{kind}') & (df.tag == 'epoch_loss')]\n",
    "    base['run'] = base.run.str.replace(f\"/{kind}\", \"\")\n",
    "\n",
    "    # Filter model\n",
    "    models_by_date = { \n",
    "        'mobnet_v3_size128_tiles1_sample1':\n",
    "            [\n",
    "                'mobnet_v3_20230908-203439',\n",
    "                'mobnet_v3_20230909-020833',\n",
    "                'mobnet_v3_20230909-121048',\n",
    "                'mobnet_v3_20230909-153231',\n",
    "                'mobnet_v3_20230909-202517',\n",
    "                'mobnet_v3_20230909-213040',\n",
    "                'mobnet_v3_20230910-130103',\n",
    "                'mobnet_v3_20230911-092415',\n",
    "                'mobnet_v3_20230911-231611',\n",
    "                'mobnet_v3_20230912-012855',\n",
    "            ],\n",
    "        'mobnet_v3_size256_tiles1_sample1':\n",
    "            [       \n",
    "                'mobnet_v3_20230914-215006',\n",
    "                'mobnet_v3_20230913-200010',\n",
    "            ],      \n",
    "        'mobnet_v3_size512_tiles1_sample1':\n",
    "            [\n",
    "                'mobnet_v3_20230915-122541',\n",
    "                'mobnet_v3_20230916-124818',\n",
    "            ],\n",
    "        'mobnet_v3_size128_tiles2_sample20':\n",
    "            [\n",
    "                'mobnet_v3_20230918-192554',\n",
    "                'mobnet_v3_20230918-231134',\n",
    "                'mobnet_v3_20230919-104912',\n",
    "                'mobnet_v3_20230919-234505',\n",
    "                'mobnet_v3_20230920-131123',\n",
    "                'mobnet_v3_20230920-180458',\n",
    "                'mobnet_v3_20230920-194725',\n",
    "            ]\n",
    "    }\n",
    "\n",
    "    base['model'] = np.nan\n",
    "    for model, names in models_by_date.items():\n",
    "        for name in names:\n",
    "            base.loc[base.run.str.contains(name), 'model'] = model\n",
    "            \n",
    "    # Reshape data\n",
    "    plot_data = base[~base.model.isna()]\\\n",
    "        .drop_duplicates(subset=['model','step'], keep='first')\\\n",
    "        .pivot(index='step', columns='model', values='value')\n",
    "        \n",
    "    # Export data\n",
    "    path_repo = r\"D:/Maestría/Tesis/Repo\"\n",
    "    plot_data.to_csv(f\"{path_repo}/data/data_out/{kind}_by_epoch.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "path = r\"/mnt/d/Maestría/Tesis/Repo/data/data_out/test_datasets/test_size128_tiles1_stacked1\"\n",
    "files = os.listdir(path)\n",
    "\n",
    "data = {}\n",
    "for file in tqdm(files):\n",
    "    link = file[5:14]\n",
    "    data[link] = np.load(fr\"{path}/{file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"test_060283610.npy\"[5:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "655360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import utils\n",
    "import build_dataset as bd\n",
    "import run_model\n",
    "\n",
    "importlib.reload(run_model)\n",
    "importlib.reload(utils)\n",
    "\n",
    "path_repo = r\"/mnt/d/Maestría/Tesis/Repo/\"\n",
    "\n",
    "log_dir = f\"{path_repo}/logs/mobnet_v3_20230905-231447\"\n",
    "models_dir = f\"{path_repo}/data/data_out/models_by_epoch\"\n",
    "model_name = \"mobnet_v3\"\n",
    "metadata = pd.read_csv(f\"{path_repo}/data/data_out/size128_sample10/metadata.csv\")\n",
    "tiles = 1\n",
    "size = 128\n",
    "resizing_size = 128\n",
    "bias = 2\n",
    "sample = 1\n",
    "to8bit = True\n",
    "n_epochs = 20\n",
    "\n",
    "\n",
    "# modelpath = rf\"{path_repo}/data/data_out/models_by_epoch/mobnet_v3_25\"\n",
    "# model = tf.keras.models.load_model(\n",
    "#            modelpath, compile=True\n",
    "#         )\n",
    "# metadata = metadata.loc[metadata.type == \"test\", \"link\"]\n",
    "# df_prediciones, mse = run_model.compute_custom_loss(\n",
    "#     model, metadata, tiles, size, resizing_size, bias, sample, to8bit\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "path_pansharpened = r\"D:\\Maestría\\Tesis\\Repo\\data\\data_in\\Pansharpened\\2013\"\n",
    "files = os.listdir(path_pansharpened)\n",
    "tifs = [f for f in files if f.endswith(\".tif\")]\n",
    "tifs\n",
    "\n",
    "ds1 = xr.open_dataset(rf\"{path_pansharpened}\\{tifs[0]}\")\n",
    "ds2 = xr.open_dataset(rf\"{path_pansharpened}\\{tifs[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.combine_by_coords([ds1, ds2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prediciones, mse = run_model.compute_custom_loss(\n",
    "    model, metadata[metadata.link == int(64100610)], tiles, size, resizing_size, bias, sample, to8bit\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import build_dataset\n",
    "import run_model\n",
    "import utils\n",
    "import matplotlib\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely import Polygon\n",
    "importlib.reload(build_dataset)\n",
    "importlib.reload(utils)\n",
    "\n",
    "# # Load datasets\n",
    "xr_datasets, extents = build_dataset.load_satellite_datasets()\n",
    "icpag = build_dataset.load_icpag_dataset()\n",
    "icpag = build_dataset.assign_links_to_datasets(icpag, extents, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icpag.iloc[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load(r\"/mnt/d/Maestría/Tesis/Repo/outputs/train_example_0_imgs.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.stacked_image_from_census_tract(xr_datasets[icpag.iloc[100].dataset], icpag.iloc[100].geometry, img_size=100, n_bands=4, stacked_images=[1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(run_model)\n",
    "# import run_model\n",
    "# test = run_model.create_datasets(image_size=512, resizing_size=128, batch_size=1, save_examples=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO:\n",
    "# 1) Agregar visuzliación de imahgenes\n",
    "# 2) Split train test bien \n",
    "# 3) Ver si se puede optimizar random_image_from_census_tract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size=512\n",
    "resizing_size=128\n",
    "batch_size=1\n",
    "small_sample=False\n",
    "save_examples=True\n",
    "\n",
    "# Based on: https://medium.com/@acordier/tf-data-dataset-generators-with-parallelization-the-easy-way-b5c5f7d2a18\n",
    "\n",
    "def get_data(i):\n",
    "    # Decoding from the EagerTensor object. Extracts the number/value from the tensor\n",
    "    #   example: <tf.Tensor: shape=(), dtype=uint8, numpy=20> -> 20\n",
    "    i = i.numpy()\n",
    "\n",
    "    # Get link, dataset and indicator value of the corresponding index\n",
    "    link = df_train.iloc[i][\"link\"]\n",
    "    value = df_train.iloc[i][\"var\"]\n",
    "    link_dataset = xr_datasets[df_train.iloc[i][\"dataset\"]]\n",
    "\n",
    "    # Generate the image\n",
    "    image, point, bounds, total_bounds = utils.random_image_from_census_tract(\n",
    "        link_dataset, df_train, link, tiles=g_tiles, size=g_image_size, bias=2\n",
    "    )\n",
    "    \n",
    "    if image is not None:\n",
    "        # Process image\n",
    "        image = np.moveaxis(image, 0, 2)\n",
    "\n",
    "        # Augment some shit\n",
    "        # ... TODO\n",
    "        #   Resize (g_resize)\n",
    "\n",
    "        # Assert that data corresponds to train or test\n",
    "        is_correct_type = build_dataset.assert_train_test_datapoint(\n",
    "            point, total_bounds, wanted_type=\"train\"\n",
    "        )\n",
    "        if is_correct_type == False:  # If the point is not train/test, discard it\n",
    "            image = 0\n",
    "            value = np.nan\n",
    "            print(\"Imagen eliminada por type\")\n",
    "\n",
    "    else:\n",
    "        # Value nan gets filtered later!\n",
    "        image = 0\n",
    "        value = np.nan\n",
    "        print(\"Imagen eliminada por None\")\n",
    "        \n",
    "    return image, value\n",
    "\n",
    "def get_train_data(i):\n",
    "    image, value = get_data(i, df_train)\n",
    "    return image, value\n",
    "\n",
    "def get_test_data(i):\n",
    "    image, value = get_data(i, df_test)\n",
    "    return image, value\n",
    "\n",
    "def _fixup_shape(x, y):\n",
    "    \"\"\"Note that you may need to add the following mapping right after batching, since in some cases\n",
    "    (depending on the layers used in the trained model and your version of TensorFlow) the implicit\n",
    "    inferring of the shapes of the output Tensors can fail due to the use of .from_generator()\n",
    "    \"\"\"\n",
    "    x.set_shape([None, None, None, 4])  # n, h, w, c\n",
    "    y.set_shape([None, 1])  # n, nb_classes\n",
    "    return x, y\n",
    "\n",
    "g_tiles = 1\n",
    "g_image_size = 512\n",
    "\n",
    "### Open dataframe with files and labels\n",
    "print(\"Reading dataset...\")\n",
    "xr_datasets, extents = build_dataset.load_satellite_datasets()\n",
    "df = build_dataset.load_icpag_dataset()\n",
    "df = build_dataset.assign_links_to_datasets(df, extents, verbose=True)\n",
    "print(\"Dataset loaded!\")\n",
    "\n",
    "print(\"Cleaning dataset...\")\n",
    "# Clean dataframe and create datasets\n",
    "# df = df[df[\"sample\"] <= 2]\n",
    "if small_sample:\n",
    "    df = df.sample(2000, random_state=825).reset_index(drop=True)\n",
    "\n",
    "### Split census tracts based on train/test\n",
    "#       (the hole census tract must be in the corresponding region)\n",
    "df[\"min_x\"] = df.bounds[\"minx\"]\n",
    "df[\"max_x\"] = df.bounds[\"maxx\"]\n",
    "df = build_dataset.split_train_test(df)\n",
    "df = df[[\"link\", \"dataset\", \"var\", \"type\", \"geometry\"]]\n",
    "\n",
    "### Train/Test\n",
    "list_of_datasets = []\n",
    "\n",
    "print()\n",
    "print(\"Benchmarking MSE against the mean\")\n",
    "\n",
    "## Train\n",
    "df_train = df[df[\"type\"] == \"train\"].copy().reset_index(drop=True)\n",
    "assert df_train.shape[0] > 0, f\"Empty train dataset!\"\n",
    "\n",
    "print(f\"Train MSE: {df_train['var'].var()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Generator for the index\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: list(\n",
    "        range(df_train.shape[0])\n",
    "    ),  # The index generator, \n",
    "    tf.uint8,\n",
    ")  # Creates a dataset with all the indexes (0, 1, 2, 3, etc.)\n",
    "\n",
    "train_dataset = train_dataset.shuffle(\n",
    "    buffer_size=50, seed=825, reshuffle_each_iteration=True\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda i: tf.py_function(  # The actual data generator. Passes the index to the function that will process the data.\n",
    "        func=get_data, inp=[i], Tout=[tf.uint8, tf.float32]\n",
    "    ), \n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    ")\n",
    "\n",
    "train_dataset = (\n",
    "    train_dataset.filter(\n",
    "        lambda img, value: (value < 0) | (value >= 0)\n",
    "    )  # Filter out NaN values\n",
    "    .batch(64)  # .map(_fixup_shape)\n",
    "    # .prefetch(tf.data.AUTOTUNE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = np.load(r\"D:\\Maestría\\Tesis\\Repo\\outputs\\test_example_3_imgs.npy\")\n",
    "imgs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = np.load(rf\"D:\\Maestría\\Tesis\\Repo\\outputs\\train_example_2_imgs.npy\")\n",
    "imgs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(18, 18), dpi=300)\n",
    "\n",
    "for pos in range(9):\n",
    "    ax = plt.subplot(3, 3, pos + 1)\n",
    "    img = utils.augment_image(imgs[0])[:,:,:3]\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug[95:105,55:65,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.fliplr(img[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3*1024/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = train_dataset.take(1)\n",
    "lista = list(d.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "import earthpy.plot as ep    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = lista[0][0][0]\n",
    "img = skimage.exposure.equalize_hist(img) # stretch\n",
    "img = np.moveaxis(img, 2, 0)\n",
    "ep.plot_rgb(img,\n",
    "        rgb=[0, 1, 2],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "import earthpy.plot as ep    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = get_data(39)[0]\n",
    "img = skimage.exposure.equalize_hist(img) # stretch\n",
    "img = np.moveaxis(img, 2, 0)\n",
    "ep.plot_rgb(img,\n",
    "        rgb=[0, 1, 2],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "import earthpy.plot as ep    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(2,2, figsize=(16, 16), facecolor='w', edgecolor='k', dpi=200)\n",
    "# fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "axs = axs.ravel()\n",
    "\n",
    "i=0\n",
    "for x in test.take(1):\n",
    "    img = x[0].numpy()[0]\n",
    "    img = skimage.exposure.equalize_hist(img) # stretch\n",
    "    img = np.moveaxis(img, 2, 0)\n",
    "    ep.plot_rgb(img,\n",
    "            rgb=[0, 1, 2],\n",
    "            ax=axs[i])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = list(range(len([2] * 30)))\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5\n",
    "\n",
    "def test1():\n",
    "    a = 3\n",
    "    test2()\n",
    "    \n",
    "def test2():\n",
    "    print(a)\n",
    "    \n",
    "\n",
    "test1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "import earthpy.plot as ep    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(2,2, figsize=(16, 16), facecolor='w', edgecolor='k', dpi=200)\n",
    "# fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "axs = axs.ravel()\n",
    "\n",
    "i=0\n",
    "for x in test.take(4):\n",
    "    img = x[0].numpy()[0]\n",
    "    img = skimage.exposure.equalize_hist(img) # stretch\n",
    "    img = np.moveaxis(img, 2, 0)\n",
    "    ep.plot_rgb(img,\n",
    "            rgb=[0, 1, 2],\n",
    "            ax=axs[i])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=10\n",
    "link = training_set.iloc[i][\"link\"]\n",
    "value = training_set.iloc[i][\"var\"]\n",
    "link_dataset = datasets[icpag.iloc[i][\"dataset\"]]\n",
    "image = utils.random_image_from_census_tract(\n",
    "    link_dataset, training_set, link, tiles=tiles, size=actual_size, bias=bias, image_return_only=True\n",
    ")    \n",
    "image = np.moveaxis(image, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icpag.loc[icpag.link == link, \"dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = icpag\n",
    "link = \"068613306\"\n",
    "tiles = 1\n",
    "actual_size = 128\n",
    "bias = 2\n",
    "\n",
    "\n",
    "link_dataset = datasets[icpag.loc[icpag.link == link, \"dataset\"].values[0]]\n",
    "image = utils.random_image_from_census_tract(\n",
    "    link_dataset, df, link, tiles=tiles, size=actual_size, bias=bias, image_return_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in dataset:\n",
    "    print(x.numpy())\n",
    "    if x==20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=10\n",
    "batch_size=32\n",
    "icpag[index * batch_size:(index + 1) * batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traingen = CustomDataGen(train_df,\n",
    "                         path='filename',\n",
    "                         value='value',\n",
    "                         batch_size=32, input_size=target_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import build_dataset\n",
    "import run_model\n",
    "import utils\n",
    "import matplotlib\n",
    "import geopandas as gpd\n",
    "from shapely import Polygon\n",
    "importlib.reload(build_dataset)\n",
    "importlib.reload(utils)\n",
    "\n",
    "# Load datasets\n",
    "datasets, extents = build_dataset.load_satellite_datasets()\n",
    "icpag = build_dataset.load_icpag_dataset()\n",
    "icpag = build_dataset.assign_links_to_datasets(icpag, extents, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['pansharpened_6741387101_R1C1'].drop_vars(\"spatial_ref\")[['band', 'y', 'x', 'band_data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['pansharpened_6741387101_R1C2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extents['pansharpened_6741387101_R1C1'].boundary.xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(extents['pansharpened_6741387101_R1C1'].boundary.xy[0], extents['pansharpened_6741387101_R1C1'].boundary.xy[1])\n",
    "plt.plot(extents['pansharpened_6741387101_R1C1'].buffer(-0.01).boundary.xy[0], extents['pansharpened_6741387101_R1C1'].buffer(-0.01).boundary.xy[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "poly = extents['pansharpened_6741387101_R1C1'].buffer(0.1)\n",
    "\n",
    "points_list = [(poly.boundary.xy[1][i], poly.boundary.xy[0][i]) for i in range(0, 4)]\n",
    "# Create a map object centered at a specific location\n",
    "m = folium.Map(location=[poly.centroid.y, poly.centroid.x], zoom_start=12)\n",
    "\n",
    "\n",
    "# Create a polygon and add it to the map\n",
    "polygon = folium.Polygon(locations=points_list, color='blue', fill=True, fill_color='red', fill_opacity=0.6)\n",
    "polygon.add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import build_dataset\n",
    "import run_model\n",
    "import utils\n",
    "import matplotlib\n",
    "import geopandas as gpd\n",
    "from shapely import Polygon\n",
    "importlib.reload(build_dataset)\n",
    "importlib.reload(utils)\n",
    "\n",
    "link = '064100610'\n",
    "\n",
    "# Load datasets\n",
    "datasets, extents = build_dataset.load_satellite_datasets()\n",
    "icpag = build_dataset.load_icpag_dataset()\n",
    "icpag = build_dataset.assign_links_to_datasets(icpag, extents, verbose=False)\n",
    "\n",
    "# Get the dataset with the images of the selected link\n",
    "ds = build_dataset.get_dataset_for_link(icpag, datasets, link)\n",
    "\n",
    "# Get the grid of the images\n",
    "images, points, bounds = build_dataset.get_gridded_images_for_link(\n",
    "    ds, icpag, link, tiles=1, size=128, resizing_size=128, bias=4, sample=1, to8bit=True\n",
    ")\n",
    "\n",
    "# Compute loss\n",
    "df_prediciones, mse = run_model.compute_custom_loss(\n",
    "    model, metadata[metadata.link == int(link)], tiles, size, resizing_size, bias, sample, to8bit\n",
    ")\n",
    "\n",
    "# Make geodataframe with the images and its predictions\n",
    "polygons = [Polygon(bound[0]) for bound in bounds]\n",
    "predictions = df_prediciones[df_prediciones['link']==link].predictions.values[0]\n",
    "predictions_gdf =  gpd.GeoDataFrame(predictions, geometry=polygons).rename(columns={0:'predictions'}).set_crs(epsg=4326)\n",
    "\n",
    "#### Plot\n",
    "import folium\n",
    "# Plot census tract\n",
    "m = icpag[icpag.link == link].explore(\n",
    "        tiles=\"https://services.arcgisonline.com/arcgis/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\", attr=\"ESRI\",\n",
    "        \n",
    ")\n",
    "# Plot gridded predictions\n",
    "predictions_gdf.explore(column='predictions', cmap='Spectral', vmin=-1, vmax=1, m=m)\n",
    "\n",
    "# Add control for switching between layers\n",
    "folium.LayerControl().add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import run_model\n",
    "import pandas as pd\n",
    "import importlib\n",
    "importlib.reload(run_model)\n",
    "\n",
    "path_repo = r\"/mnt/d/Maestría/Tesis/Repo/\"\n",
    "modelpath = rf\"{path_repo}/data/data_out/models_by_epoch/mobnet_v3_25\"\n",
    "# model = tf.keras.models.load_model(\n",
    "#            modelpath, compile=True\n",
    "#         )\n",
    "# metadata = pd.read_csv(f\"{path_repo}/data/data_out/train_size128_tiles1_sample10/metadata.csv\")\n",
    "tiles, size, resizing_size, bias, sample = 1, 128, 128, 2, 1\n",
    "\n",
    "df, mse = run_model.compute_custom_loss(\n",
    "    model, metadata, tiles, size, resizing_size, bias, sample, True, verbose=True\n",
    ")a\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.loc[metadata.link ==10 ].icpag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_gdf.explore(column='predictions', cmap='Spectral', vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.concatenate([np.array([10,20,30]),np.array([10])]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorize(prediction, vmin=vmin, vmax=vmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "metadata = pd.read_csv(r\"D:\\Maestría\\Tesis\\Repo\\data\\data_out\\size512_sample5\\metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 20010101\n",
    "metadata.loc[metadata.link == link, 'var'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"./data/icpag.csv\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "path_dataout = r\"D:\\Maestría\\Tesis\\Repo\\data\\data_out\"\n",
    "model_path=fr\"{path_dataout}/models/mobnet_v3_20230831-172738\"\n",
    "\n",
    "keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "img = np.load(r\"D:\\Maestría\\Tesis\\Repo\\data\\data_out\\size128_sample10\\020010103_1.npy\")\n",
    "img = np.moveaxis(img, 0, 2)[:,:,:3]\n",
    "# equalize hist\n",
    "img = skimage.exposure.equalize_hist(img)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import earthpy.plot as ep\n",
    "\n",
    "# ep.plot_rgb(images[0], rgb=[0, 1, 2], title=\"RGB Image\", stretch=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import dotenv_values\n",
    "image_size = 200\n",
    "sample_size = 1\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "env = dotenv_values(\"D:/Maestría/Tesis/Repo/scripts/globals.env\")\n",
    "\n",
    "path_proyecto = env[\"PATH_PROYECTO\"]\n",
    "path_datain = env[\"PATH_DATAIN\"]\n",
    "path_dataout = env[\"PATH_DATAOUT\"]\n",
    "path_scripts = env[\"PATH_SCRIPTS\"]\n",
    "path_satelites = env[\"PATH_SATELITES\"]\n",
    "path_logs = env[\"PATH_LOGS\"]\n",
    "path_outputs = env[\"PATH_OUTPUTS\"]\n",
    "\n",
    "# df = pd.read_csv(\n",
    "#     rf\"D:\\Maestría\\Tesis\\Repo\\data\\data_out\\size500_sample1\\metadata.csv\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import build_dataset\n",
    "importlib.reload(build_dataset)\n",
    "variable = \"ln_pred_inc_mean\"\n",
    "datasets, extents = build_dataset.load_satellite_datasets()\n",
    "icpag = build_dataset.load_icpag_dataset(variable)\n",
    "icpag = build_dataset.assign_links_to_datasets(icpag, extents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import run_model\n",
    "import importlib\n",
    "importlib.reload(run_model)\n",
    "\n",
    "kind=\"reg\"\n",
    "image_size = 500\n",
    "resizing_size = 200\n",
    "my_test = \"ddddddd\"\n",
    "train_dataset, test_dataset, filenames = run_model.create_and_build_datasets(\n",
    "    kind=kind,\n",
    "    image_size=image_size,\n",
    "    resizing_size=resizing_size,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icpag['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "imgs = np.load(r\"D:\\Maestría\\Tesis\\Repo\\outputs\\examples_0_img.npy\")\n",
    "labs = np.load(r\"D:\\Maestría\\Tesis\\Repo\\outputs\\examples_0_lab.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(file_path, label):\n",
    "    import cv2\n",
    "    \n",
    "    img = np.load(file_path)\n",
    "    img = np.moveaxis(\n",
    "        img, 0, 2\n",
    "    )  # Move axis so the original [4, 512, 512] becames [512, 512, 4]\n",
    "    img = cv2.resize(\n",
    "        img, dsize=(resizing_size, resizing_size), interpolation=cv2.INTER_CUBIC\n",
    "    )\n",
    "\n",
    "    img = tf.convert_to_tensor(img / 255, dtype=tf.float32)\n",
    "    label = tf.cast(label, tf.float32)\n",
    "\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "resizing_size = 200\n",
    "img, label = process_image(r\"D:\\Maestría\\Tesis\\Repo\\data\\data_out\\size500_sample1\\062742004_0.npy\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "example = tfds.as_numpy(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(example[:,:,0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = r\"D:\\Maestría\\Tesis\\Repo\\data\\data_out\\size500_sample1\"\n",
    "files = [f\"{path}\\{file}\" for file in os.listdir(path)]\n",
    "\n",
    "labels = [0] * len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((files, labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "    lambda file, label: tf.numpy_function(\n",
    "        process_image, [file, label], (tf.float32, tf.float32)\n",
    "    )\n",
    ")  # Parse every image in the dataset using `map`\n",
    "\n",
    "imgs = []\n",
    "labs = []\n",
    "for x in dataset.take(1):\n",
    "    imgs += [tfds.as_numpy(x)[0]]\n",
    "    labs += [tfds.as_numpy(x)[1]]\n",
    "    \n",
    "imgs\n",
    "#### HASTA ACA TODO GENIAL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "        [\n",
    "            layers.RandomFlip(\n",
    "                \"horizontal_and_vertical\",\n",
    "                seed=825,\n",
    "                input_shape=(resizing_size, resizing_size, 4),\n",
    "            ),\n",
    "            # layers.RandomTranslation(0.3, 0.3, fill_mode=\"reflect\", seed=825),\n",
    "            # layers.RandomHeight(0.3),\n",
    "            # layers.RandomWidth(0.3),\n",
    "            # layers.RandomZoom(0.3, seed=825),\n",
    "            # layers.RandomContrast(0.3, seed=825),\n",
    "            # layers.RandomBrightness(0.05, value_range=(0,1), seed=825),\n",
    "            # layers.RandomCrop(image_size, image_size, seed=825),\n",
    "        ],\n",
    "        name=\"data_augmentation\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    dataset.shuffle(round(len(files[0]) / 10))\n",
    "    .batch(32)\n",
    "    .map(lambda x, y: (data_augmentation(x), y))\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "ds = datasets[icpag.loc[icpag.link == \"065150214\", \"dataset\"].values[0]]\n",
    "composition, point, boundaries, total_boundaries = utils.random_image_from_census_tract(\n",
    "    ds, icpag, \"065150214\", tiles=2, size=500, bias=4, to8bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "labs = []\n",
    "for x in train_dataset.take(1):\n",
    "    imgs += [tfds.as_numpy(x)[0]]\n",
    "    labs += [tfds.as_numpy(x)[1]]\n",
    "\n",
    "print(imgs[0][0].shape)    \n",
    "plt.imshow(imgs[0][0][:,:,:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1\n",
    "image_size = 500\n",
    "path_dataout = r\"D:/Maestría/Tesis/Repo/data/data_out\"\n",
    "\n",
    "df = pd.read_csv(\n",
    "    rf\"{path_dataout}/size{image_size}_sample{sample_size}/metadata.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_column_to_x_y(df):\n",
    "    df.point = df.point.str.replace(\"\\(|\\)\", \"\", regex=True).str.split(\",\")\n",
    "    df['x'] = df.point.str[0]\n",
    "    df['y']= df.point.str[1]\n",
    "    return df[['x', 'y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "df = df.dropna(how=\"any\").reset_index(drop=True)\n",
    "df[['x', 'y']] = point_column_to_x_y(df)\n",
    "df.x, df.y = df.x.astype(float), df.y.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import build_dataset\n",
    "import pandas as pd\n",
    "path_dataout = r\"D:/Maestría/Tesis/Repo/data/data_out\"\n",
    "\n",
    "# build_dataset.build_dataset(200, 1, variable=\"ln_pred_inc_mean\")\n",
    "df = pd.read_csv(\n",
    "    rf\"{path_dataout}/size200_sample1/metadata.csv\"\n",
    ")\n",
    "\n",
    "metadata = build_dataset.split_train_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import build_dataset\n",
    "import utils\n",
    "importlib.reload(build_dataset)\n",
    "importlib.reload(utils)\n",
    "importlib.reload(build_dataset)\n",
    "build_dataset.build_dataset(500, 1, tiles=2, bias=4, variable=\"ln_pred_inc_mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(build_dataset)\n",
    "df = pd.read_csv(\n",
    "    rf\"{path_dataout}/size500_sample1/metadata.csv\"\n",
    ")\n",
    "# df = df.drop(columns=\"Test\")\n",
    "# df = build_dataset.split_train_test(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
