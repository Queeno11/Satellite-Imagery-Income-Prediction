{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import run_model\n",
    "import build_dataset\n",
    "import utils\n",
    "import xarray as xr\n",
    "\n",
    "files = [r\"D:\\Maestría\\Tesis\\Repo\\data\\data_in\\Compressed\\2013\\pansharpened_6741392101_R2C1.tif\", r\"D:\\Maestría\\Tesis\\Repo\\data\\data_in\\Compressed\\2013\\pansharpened_6741392101_R2C2.tif\"]\n",
    "datasets = {\n",
    "        f.split(\"\\\\\")[-1].replace(\".tif\", \"\"): (xr.open_dataset(f))\n",
    "        for f in files\n",
    "}\n",
    "extents = {name: utils.get_dataset_extent(ds) for name, ds in datasets.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "img = np.load(r\"\\\\wsl.localhost\\UbuntuE\\home\\nico\\data\\test_datasets\\test_size128_tiles1_stacked1-2\\test_020010201.npy\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(2,1)\n",
    "ax[0].imshow(img[1][:,:,4:7])\n",
    "ax[1].imshow(img[1][:,:,:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img[1][:,:,:3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n",
    "\n",
    "# Define the coordinates\n",
    "coordinates = [(-58.50926393899999, -34.606511477),\n",
    "               (-58.50926393899999, -34.607101477),\n",
    "               (-58.50867393899999, -34.607101477),\n",
    "               (-58.50867393899999, -34.606511477)]\n",
    "\n",
    "# Create a shapely polygon\n",
    "polygon = Polygon(coordinates)\n",
    "\n",
    "polygon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extents[\"pansharpened_6741392101_R2C2\"].intersects(polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files[0].split(\"\\\\\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import custom_models\n",
    "\n",
    "resizing_size = 128\n",
    "models_dir = \"/mnt/\"\n",
    "model = custom_models.efficientnet_v2S(resizing_size, bands=4, kind=\"reg\", weights=None)\n",
    "model.load_weights(f\"/mnt/d/Maestría/Tesis/Repo/data/data_out/models_by_epoch/effnet_v2S_size256_tiles1_sample5/effnet_v2S_size256_tiles1_sample5_100/variables/variables\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(0.75 + np.random.rand(1000) / 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage import io, color\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "imgs = np.load(r\"D:\\Maestría\\Tesis\\Repo\\outputs\\mobnet_v3_large_size256_tiles1_sample5_maybe_working_test_example_3_imgs.npy\")\n",
    "img = imgs[2,:,:,:3]\n",
    "\n",
    "# Increase the saturation\n",
    "# You can adjust the factor to increase or decrease the saturation\n",
    "saturation_factor = 0.5 + np.random.rand() # You can adjust this value\n",
    " \n",
    "img_hsv = color.rgb2hsv(img) # hue-saturation-value\n",
    "img_hsv[:, :, 1] *= saturation_factor\n",
    "img_hsv[:, :, 1] = np.clip(img_hsv[:, :, 1], 0, 255)\n",
    "img = color.hsv2rgb(img_hsv)\n",
    "\n",
    "# Convert the image back to RGB color space\n",
    "\n",
    "# Display the original and the saturation increased image\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axes[0].imshow(img)\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(image_saturation_increased)\n",
    "axes[1].set_title('Saturation Increased Image')\n",
    "axes[1].axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "size128_stacked1 = np.load(r\"\\\\wsl.localhost\\UbuntuE\\home\\nico\\data\\test_datasets\\test_size128_tiles1_stacked1\\valid_links.npy\")\n",
    "size128_stacked14 = np.load(r\"\\\\wsl.localhost\\UbuntuE\\home\\nico\\data\\test_datasets\\test_size128_tiles1_stacked1-4\\valid_links.npy\")test_size512_tiles1_stacked1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size128_stacked1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size128_stacked14.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMaestría\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTesis\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mRepo\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata_out\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodels_by_epoch\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124meffnet_v2S_lr0.0001_size128_y2013-2018-2022_stack1-4\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124meffnet_v2S_lr0.0001_size128_y2013-2018-2022_stack1-4_141.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\__init__.py:48\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore # noqa:F401\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     ArrowDtype,\n\u001b[0;32m     51\u001b[0m     Int8Dtype,\n\u001b[0;32m     52\u001b[0m     Int16Dtype,\n\u001b[0;32m     53\u001b[0m     Int32Dtype,\n\u001b[0;32m     54\u001b[0m     Int64Dtype,\n\u001b[0;32m     55\u001b[0m     UInt8Dtype,\n\u001b[0;32m     56\u001b[0m     UInt16Dtype,\n\u001b[0;32m     57\u001b[0m     UInt32Dtype,\n\u001b[0;32m     58\u001b[0m     UInt64Dtype,\n\u001b[0;32m     59\u001b[0m     Float32Dtype,\n\u001b[0;32m     60\u001b[0m     Float64Dtype,\n\u001b[0;32m     61\u001b[0m     CategoricalDtype,\n\u001b[0;32m     62\u001b[0m     PeriodDtype,\n\u001b[0;32m     63\u001b[0m     IntervalDtype,\n\u001b[0;32m     64\u001b[0m     DatetimeTZDtype,\n\u001b[0;32m     65\u001b[0m     StringDtype,\n\u001b[0;32m     66\u001b[0m     BooleanDtype,\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     NA,\n\u001b[0;32m     69\u001b[0m     isna,\n\u001b[0;32m     70\u001b[0m     isnull,\n\u001b[0;32m     71\u001b[0m     notna,\n\u001b[0;32m     72\u001b[0m     notnull,\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     Index,\n\u001b[0;32m     75\u001b[0m     CategoricalIndex,\n\u001b[0;32m     76\u001b[0m     RangeIndex,\n\u001b[0;32m     77\u001b[0m     MultiIndex,\n\u001b[0;32m     78\u001b[0m     IntervalIndex,\n\u001b[0;32m     79\u001b[0m     TimedeltaIndex,\n\u001b[0;32m     80\u001b[0m     DatetimeIndex,\n\u001b[0;32m     81\u001b[0m     PeriodIndex,\n\u001b[0;32m     82\u001b[0m     IndexSlice,\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[0;32m     84\u001b[0m     NaT,\n\u001b[0;32m     85\u001b[0m     Period,\n\u001b[0;32m     86\u001b[0m     period_range,\n\u001b[0;32m     87\u001b[0m     Timedelta,\n\u001b[0;32m     88\u001b[0m     timedelta_range,\n\u001b[0;32m     89\u001b[0m     Timestamp,\n\u001b[0;32m     90\u001b[0m     date_range,\n\u001b[0;32m     91\u001b[0m     bdate_range,\n\u001b[0;32m     92\u001b[0m     Interval,\n\u001b[0;32m     93\u001b[0m     interval_range,\n\u001b[0;32m     94\u001b[0m     DateOffset,\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     to_numeric,\n\u001b[0;32m     97\u001b[0m     to_datetime,\n\u001b[0;32m     98\u001b[0m     to_timedelta,\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     Flags,\n\u001b[0;32m    101\u001b[0m     Grouper,\n\u001b[0;32m    102\u001b[0m     factorize,\n\u001b[0;32m    103\u001b[0m     unique,\n\u001b[0;32m    104\u001b[0m     value_counts,\n\u001b[0;32m    105\u001b[0m     NamedAgg,\n\u001b[0;32m    106\u001b[0m     array,\n\u001b[0;32m    107\u001b[0m     Categorical,\n\u001b[0;32m    108\u001b[0m     set_eng_float_format,\n\u001b[0;32m    109\u001b[0m     Series,\n\u001b[0;32m    110\u001b[0m     DataFrame,\n\u001b[0;32m    111\u001b[0m )\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[1;32mc:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\api.py:27\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     isna,\n\u001b[0;32m     17\u001b[0m     isnull,\n\u001b[0;32m     18\u001b[0m     notna,\n\u001b[0;32m     19\u001b[0m     notnull,\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     factorize,\n\u001b[0;32m     24\u001b[0m     unique,\n\u001b[0;32m     25\u001b[0m     value_counts,\n\u001b[0;32m     26\u001b[0m )\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Categorical\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowDtype\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mboolean\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BooleanDtype\n",
      "File \u001b[1;32mc:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\arrays\\__init__.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfloating\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FloatingArray\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minteger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IntegerArray\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IntervalArray\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmasked\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseMaskedArray\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy_\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PandasArray\n",
      "File \u001b[1;32mc:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\arrays\\interval.py:90\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     86\u001b[0m     ExtensionArray,\n\u001b[0;32m     87\u001b[0m     _extension_array_shared_docs,\n\u001b[0;32m     88\u001b[0m )\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatetimes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatetimeArray\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtimedeltas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TimedeltaArray\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcom\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstruction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     93\u001b[0m     array \u001b[38;5;28;01mas\u001b[39;00m pd_array,\n\u001b[0;32m     94\u001b[0m     ensure_wrapped_if_datetimelike,\n\u001b[0;32m     95\u001b[0m     extract_array,\n\u001b[0;32m     96\u001b[0m )\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1138\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1078\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1504\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1476\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1612\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r\"D:\\Maestría\\Tesis\\Repo\\data\\data_out\\models_by_epoch\\effnet_v2S_lr0.0001_size128_y2013-2018-2022_stack1-4\\effnet_v2S_lr0.0001_size128_y2013-2018-2022_stack1-4_141.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the non-masked region (where values are not zero)\n",
    "non_masked = ds.where(ds != 0)\n",
    "\n",
    "# Get the bounding box of the non-masked region\n",
    "min_x, max_x = non_masked.x.min(), non_masked.x.max()\n",
    "min_y, max_y = non_masked.y.min(), non_masked.y.max()\n",
    "\n",
    "# Subset the xarray dataset to the non-masked region\n",
    "subset_ds = ds.sel(x=slice(min_x, max_x), y=slice(min_y, max_y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import build_dataset\n",
    "datasets, extents = build_dataset.load_landsat_datasets()\n",
    "icpag = build_dataset.load_icpag_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "image = np.load(r\"D:/Maestría/Tesis/Repo/outputs/mobnet_v3_large_size128_tiles1_sample1_maybe_working_test_example_4_imgs.npy\")\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(10,5))\n",
    "axs[0].imshow(image[0,:,:,0:3])\n",
    "axs[1].imshow(image[0,:,:,4:7])\n",
    "axs[0].set_axis_off()\n",
    "axs[1].set_axis_off()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icpag[\"dataset_2013\"] = \"LC08_225084_20130725\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets, extents = build_dataset.load_landsat_datasets()\n",
    "\n",
    "ds = datasets[\"LC08_225084_20130725\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = icpag\n",
    "all_years_datasets = {}\n",
    "all_years_datasets[2013] = datasets\n",
    "import random\n",
    "import utils\n",
    "nbands=11\n",
    "stacked_images = [1]\n",
    "image_size=32\n",
    "resizing_size=32\n",
    "# def get_data(i, df_subset, type=\"train\", load=False):\n",
    "    # Decoding from the EagerTensor object. Extracts the number/value from the tensor\n",
    "    #   example: <tf.Tensor: shape=(), dtype=uint8, numpy=20> -> 20\n",
    "\n",
    "year = random.choices(\n",
    "    population=list(all_years_datasets.keys()),\n",
    "    weights=[1],\n",
    "    k=1\n",
    ")[0]\n",
    "sat_img_dataset = all_years_datasets[year]\n",
    "# initialize iterators & params\n",
    "iteration = 0\n",
    "is_correct_type = False\n",
    "image = np.zeros(shape=(nbands, 0, 0))\n",
    "total_bands = nbands * len(stacked_images)\n",
    "img_correct_shape = (total_bands, image_size, image_size)\n",
    "i=500\n",
    "\n",
    "# Get link, dataset and indicator value of the corresponding index\n",
    "polygon = df_subset.iloc[i][\"geometry\"]\n",
    "value = df_subset.iloc[i][\"var\"]\n",
    "link_dataset = sat_img_dataset[df_subset.iloc[i][f\"dataset_{year}\"]]\n",
    "\n",
    "while (is_correct_type == False) & (iteration<=5):\n",
    "\n",
    "    # Generate the image\n",
    "    image, boundaries = utils.stacked_image_from_census_tract(\n",
    "        dataset=link_dataset,\n",
    "        polygon=polygon,\n",
    "        img_size=image_size,\n",
    "        n_bands=nbands,\n",
    "        stacked_images=stacked_images,\n",
    "    )\n",
    "\n",
    "    # (1) Image has to have the correct shape\n",
    "    if image.shape == img_correct_shape:\n",
    "        # (2) Image has to fall in train or test side\n",
    "        is_correct_type = build_dataset.assert_train_test_datapoint(\n",
    "            boundaries, wanted_type=type\n",
    "        )\n",
    "\n",
    "    iteration +=1\n",
    "\n",
    "if iteration>=5:\n",
    "    print(f\"More than 5 interations for link {df_subset.iloc[i]['link']}, moving to next link...\")\n",
    "    # image = np.zeros(shape=(resizing_size, resizing_size, total_bands))\n",
    "    # value = 0\n",
    "    # return image, value\n",
    "\n",
    "# # Reduce quality and process image\n",
    "# image = utils.process_image(image, resizing_size=resizing_size)\n",
    "\n",
    "# # Augment dataset\n",
    "# if type == \"train\":\n",
    "#     image = utils.augment_image(image)\n",
    "#     # image = image\n",
    "\n",
    "# np.save(fr\"/mnt/d/Maestría/Tesis/Repo/data/data_out/test_arrays/img_{i}_{df_subset.iloc[i].link}.npy\", image)\n",
    "# return image, value\n",
    "\n",
    "# image, value = get_data(200, df_subset, type=\"train\", load=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import exposure\n",
    "import xarray as xr\n",
    "bands = ds.band.values\n",
    "x = ds.x.values\n",
    "y = ds.y.values\n",
    "sr = ds.spatial_ref.values \n",
    "band_data = exposure.equalize_hist(ds.band_data.to_numpy())\n",
    "band_data = exposure.adjust_gamma(band_data, 1.2)\n",
    "band_data = xr.DataArray(band_data, coords={'band': bands, 'x': x, \n",
    "                                'y': y, 'spatial_ref':sr},\n",
    "             dims=['band', 'y', 'x'])\n",
    "ds[\"band_data\"] = band_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import build_dataset\n",
    "\n",
    "datasets, extents = build_dataset.load_landsat_datasets()\n",
    "\n",
    "ds = datasets[\"LC08_225084_20130725\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import earthpy.plot as ep\n",
    "im = ds.isel(y=slice(0, 64), x=slice(0, 64)).band_data.astype(np.uint8).to_numpy()\n",
    "# im = utils.process_image(im, 124)\n",
    "ep.plot_rgb(im, rgb=(3,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import earthpy.plot as ep\n",
    "ep.plot_rgb(image,rgb=(3, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"LC08_225084_20130725\"].sel(band=2).band_data.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import build_dataset\n",
    "import utils\n",
    "from importlib import reload\n",
    "\n",
    "# Load datasets\n",
    "datasets, extents = build_dataset.load_satellite_datasets()\n",
    "icpag = build_dataset.load_icpag_dataset()\n",
    "icpag = build_dataset.assign_links_to_datasets(icpag, extents, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import build_dataset\n",
    "build_dataset.load_icpag_dataset(variable=\"ln_pred_inc_mean\", trim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\"D:\\Maestría\\Tesis\\Repo\\data\\data_out\\test_datasets\\test_size128_tiles1_stacked1\\test_020010205.npy\"\n",
    "train = np.load(r\"/mnt/d/Maestría/Tesis/Repo/data/data_out/test_datasets/test_size128_tiles1_stacked1/test_020010205.npy\")\n",
    "pics = train.shape[0]\n",
    "fig, axs = plt.subplots(pics, 1, figsize=(6, 6*pics))\n",
    "for i in range(pics):\n",
    "    axs[i].imshow(train[i][:,:,:3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import Point\n",
    "point = Point(10, -20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import run_model\n",
    "reload(run_model)\n",
    "run_model.get_data(10, icpag, load=True, type=\"test\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = icpag.sample(1).link.values[0]\n",
    "ds = build_dataset.get_dataset_for_link(icpag, datasets, link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icpag[icpag.link == link].geometry.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import run_model\n",
    "run_model.create_train_test_dataframes(small_sample=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import build_dataset\n",
    "import geopandas as gpd\n",
    "sat_img_datasets, extents = build_dataset.load_satellite_datasets()\n",
    "# df_test = gpd.read_feather(rf\"{path_dataout}/test_datasets/test_dataframe.feather\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "arr = np.load(r\"/mnt/d/Maestría/Tesis/Repo/data/data_out/test_datasets/test_size384_tiles1_stacked1/test_068051414.npy\")\n",
    "plt.imshow(arr[1,:,:,:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import Polygon\n",
    "P = Polygon([[0, 0], [1, 0], [1, 1], [0, 1]])\n",
    "\n",
    "P.centroid.x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "arr = np.load(r\"/mnt/d/Maestría/Tesis/Repo/outputs/test_example_1_imgs.npy\")\n",
    "plt.imshow(arr[1,:,:,:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = gpd.read_feather(rf\"/mnt/d/Maestría/Tesis/Repo/data/data_out/test_datasets/test_dataframe.feather\")\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import compute_true_loss\n",
    "import importlib\n",
    "\n",
    "compute_true_loss.compute_custom_loss_all_epochs(\n",
    "    models_dir=r\"/mnt/d/Maestría/Tesis/Repo/data/data_out/models_by_epoch/mobnet_v3_size128_tiles1_sample5\",\n",
    "    model_name=\"mobnet_v3_size128_tiles1_sample5\",\n",
    "    tiles=1,\n",
    "    size=128,\n",
    "    resizing_size=128,\n",
    "    n_epochs=200,\n",
    "    n_bands=4,\n",
    "    stacked_images=[1],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder, links = compute_true_loss.generate_gridded_images(\n",
    "    tiles=1,\n",
    "    size=128,\n",
    "    resizing_size=128,\n",
    "    n_bands=4,\n",
    "    stacked_images=[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.loc[df_test[\"link\"]==20010201, \"geometry\"].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"link\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2]\n",
    "'_'.join(str(x) for x in a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ds\n",
    "test.where(test.band_data<100,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.isel(x=slice(0,100), y=slice(0,100)) - 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.band_data.isel(x=slice(0,10000), y=slice(0,10000)).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import skimage\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,6))\n",
    "\n",
    "clip_img_a = image[:3,:,:]\n",
    "clip_img_a = np.moveaxis(clip_img_a,0,2)\n",
    "clip_img_a = skimage.exposure.equalize_hist(clip_img_a)\n",
    "ax[0].imshow(clip_img_a)\n",
    "\n",
    "clip_img_b = image[4:7,:,:]\n",
    "clip_img_b = np.moveaxis(clip_img_b,0,2)\n",
    "clip_img_b = skimage.exposure.equalize_hist(clip_img_b)\n",
    "ax[1].imshow(clip_img_b)\n",
    "clip_img_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stretch(image, minimum, maximum):\n",
    "    image = (image - minimum) / (maximum - minimum)\n",
    "    image[image < 0] = 0\n",
    "    image[image > 1] = 1\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[ds<100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds - ds.band_data.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stretch(ds, ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(dataset, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import build_dataset\n",
    "\n",
    "census_data = build_dataset.load_icpag_dataset(variable=\"ln_pred_inc_mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import run_model\n",
    "\n",
    "df_train, df_test, sat_img_dataset = run_model.create_train_test_dataframes(\n",
    "        small_sample=False\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_data.to_parquet(\"census_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "import numpy as np\n",
    "\n",
    "# Extract data from TB\n",
    "experiment_id = \"RAqyLXG2RMKkAXMmNEDV7Q\"\n",
    "experiment = tb.data.experimental.ExperimentFromDev(experiment_id)\n",
    "df = experiment.get_scalars()\n",
    "\n",
    "for kind in [\"train\", \"validation\"]:\n",
    "    # Keep only train values\n",
    "    base = df[df.run.str.contains(f'{kind}') & (df.tag == 'epoch_loss')]\n",
    "    base['run'] = base.run.str.replace(f\"/{kind}\", \"\")\n",
    "\n",
    "    # Filter model\n",
    "    models_by_date = { \n",
    "        'mobnet_v3_size128_tiles1_sample1':\n",
    "            [\n",
    "                'mobnet_v3_20230908-203439',\n",
    "                'mobnet_v3_20230909-020833',\n",
    "                'mobnet_v3_20230909-121048',\n",
    "                'mobnet_v3_20230909-153231',\n",
    "                'mobnet_v3_20230909-202517',\n",
    "                'mobnet_v3_20230909-213040',\n",
    "                'mobnet_v3_20230910-130103',\n",
    "                'mobnet_v3_20230911-092415',\n",
    "                'mobnet_v3_20230911-231611',\n",
    "                'mobnet_v3_20230912-012855',\n",
    "            ],\n",
    "        'mobnet_v3_size256_tiles1_sample1':\n",
    "            [       \n",
    "                'mobnet_v3_20230914-215006',\n",
    "                'mobnet_v3_20230913-200010',\n",
    "            ],      \n",
    "        'mobnet_v3_size512_tiles1_sample1':\n",
    "            [\n",
    "                'mobnet_v3_20230915-122541',\n",
    "                'mobnet_v3_20230916-124818',\n",
    "            ],\n",
    "        'mobnet_v3_size128_tiles2_sample20':\n",
    "            [\n",
    "                'mobnet_v3_20230918-192554',\n",
    "                'mobnet_v3_20230918-231134',\n",
    "                'mobnet_v3_20230919-104912',\n",
    "                'mobnet_v3_20230919-234505',\n",
    "                'mobnet_v3_20230920-131123',\n",
    "                'mobnet_v3_20230920-180458',\n",
    "                'mobnet_v3_20230920-194725',\n",
    "            ]\n",
    "    }\n",
    "\n",
    "    base['model'] = np.nan\n",
    "    for model, names in models_by_date.items():\n",
    "        for name in names:\n",
    "            base.loc[base.run.str.contains(name), 'model'] = model\n",
    "            \n",
    "    # Reshape data\n",
    "    plot_data = base[~base.model.isna()]\\\n",
    "        .drop_duplicates(subset=['model','step'], keep='first')\\\n",
    "        .pivot(index='step', columns='model', values='value')\n",
    "        \n",
    "    # Export data\n",
    "    path_repo = r\"D:/Maestría/Tesis/Repo\"\n",
    "    plot_data.to_csv(f\"{path_repo}/data/data_out/{kind}_by_epoch.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "path = r\"/mnt/d/Maestría/Tesis/Repo/data/data_out/test_datasets/test_size128_tiles1_stacked1\"\n",
    "files = os.listdir(path)\n",
    "\n",
    "data = {}\n",
    "for file in tqdm(files):\n",
    "    link = file[5:14]\n",
    "    data[link] = np.load(fr\"{path}/{file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"test_060283610.npy\"[5:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "655360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import utils\n",
    "import build_dataset as bd\n",
    "import run_model\n",
    "\n",
    "importlib.reload(run_model)\n",
    "importlib.reload(utils)\n",
    "\n",
    "path_repo = r\"/mnt/d/Maestría/Tesis/Repo/\"\n",
    "\n",
    "log_dir = f\"{path_repo}/logs/mobnet_v3_20230905-231447\"\n",
    "models_dir = f\"{path_repo}/data/data_out/models_by_epoch\"\n",
    "model_name = \"mobnet_v3\"\n",
    "metadata = pd.read_csv(f\"{path_repo}/data/data_out/size128_sample10/metadata.csv\")\n",
    "tiles = 1\n",
    "size = 128\n",
    "resizing_size = 128\n",
    "bias = 2\n",
    "sample = 1\n",
    "to8bit = True\n",
    "n_epochs = 20\n",
    "\n",
    "\n",
    "# modelpath = rf\"{path_repo}/data/data_out/models_by_epoch/mobnet_v3_25\"\n",
    "# model = tf.keras.models.load_model(\n",
    "#            modelpath, compile=True\n",
    "#         )\n",
    "# metadata = metadata.loc[metadata.type == \"test\", \"link\"]\n",
    "# df_prediciones, mse = run_model.compute_custom_loss(\n",
    "#     model, metadata, tiles, size, resizing_size, bias, sample, to8bit\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "path_pansharpened = r\"D:\\Maestría\\Tesis\\Repo\\data\\data_in\\Pansharpened\\2013\"\n",
    "files = os.listdir(path_pansharpened)\n",
    "tifs = [f for f in files if f.endswith(\".tif\")]\n",
    "tifs\n",
    "\n",
    "ds1 = xr.open_dataset(rf\"{path_pansharpened}\\{tifs[0]}\")\n",
    "ds2 = xr.open_dataset(rf\"{path_pansharpened}\\{tifs[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.combine_by_coords([ds1, ds2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prediciones, mse = run_model.compute_custom_loss(\n",
    "    model, metadata[metadata.link == int(64100610)], tiles, size, resizing_size, bias, sample, to8bit\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import build_dataset\n",
    "import run_model\n",
    "import utils\n",
    "import matplotlib\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely import Polygon\n",
    "importlib.reload(build_dataset)\n",
    "importlib.reload(utils)\n",
    "\n",
    "# # Load datasets\n",
    "xr_datasets, extents = build_dataset.load_satellite_datasets()\n",
    "icpag = build_dataset.load_icpag_dataset()\n",
    "icpag = build_dataset.assign_links_to_datasets(icpag, extents, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icpag.iloc[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load(r\"/mnt/d/Maestría/Tesis/Repo/outputs/train_example_0_imgs.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.stacked_image_from_census_tract(xr_datasets[icpag.iloc[100].dataset], icpag.iloc[100].geometry, img_size=100, n_bands=4, stacked_images=[1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(run_model)\n",
    "# import run_model\n",
    "# test = run_model.create_datasets(image_size=512, resizing_size=128, batch_size=1, save_examples=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO:\n",
    "# 1) Agregar visuzliación de imahgenes\n",
    "# 2) Split train test bien \n",
    "# 3) Ver si se puede optimizar random_image_from_census_tract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size=512\n",
    "resizing_size=128\n",
    "batch_size=1\n",
    "small_sample=False\n",
    "save_examples=True\n",
    "\n",
    "# Based on: https://medium.com/@acordier/tf-data-dataset-generators-with-parallelization-the-easy-way-b5c5f7d2a18\n",
    "\n",
    "def get_data(i):\n",
    "    # Decoding from the EagerTensor object. Extracts the number/value from the tensor\n",
    "    #   example: <tf.Tensor: shape=(), dtype=uint8, numpy=20> -> 20\n",
    "    i = i.numpy()\n",
    "\n",
    "    # Get link, dataset and indicator value of the corresponding index\n",
    "    link = df_train.iloc[i][\"link\"]\n",
    "    value = df_train.iloc[i][\"var\"]\n",
    "    link_dataset = xr_datasets[df_train.iloc[i][\"dataset\"]]\n",
    "\n",
    "    # Generate the image\n",
    "    image, point, bounds, total_bounds = utils.random_image_from_census_tract(\n",
    "        link_dataset, df_train, link, tiles=g_tiles, size=g_image_size, bias=2\n",
    "    )\n",
    "    \n",
    "    if image is not None:\n",
    "        # Process image\n",
    "        image = np.moveaxis(image, 0, 2)\n",
    "\n",
    "        # Augment some shit\n",
    "        # ... TODO\n",
    "        #   Resize (g_resize)\n",
    "\n",
    "        # Assert that data corresponds to train or test\n",
    "        is_correct_type = build_dataset.assert_train_test_datapoint(\n",
    "            point, total_bounds, wanted_type=\"train\"\n",
    "        )\n",
    "        if is_correct_type == False:  # If the point is not train/test, discard it\n",
    "            image = 0\n",
    "            value = np.nan\n",
    "            print(\"Imagen eliminada por type\")\n",
    "\n",
    "    else:\n",
    "        # Value nan gets filtered later!\n",
    "        image = 0\n",
    "        value = np.nan\n",
    "        print(\"Imagen eliminada por None\")\n",
    "        \n",
    "    return image, value\n",
    "\n",
    "def get_train_data(i):\n",
    "    image, value = get_data(i, df_train)\n",
    "    return image, value\n",
    "\n",
    "def get_test_data(i):\n",
    "    image, value = get_data(i, df_test)\n",
    "    return image, value\n",
    "\n",
    "def _fixup_shape(x, y):\n",
    "    \"\"\"Note that you may need to add the following mapping right after batching, since in some cases\n",
    "    (depending on the layers used in the trained model and your version of TensorFlow) the implicit\n",
    "    inferring of the shapes of the output Tensors can fail due to the use of .from_generator()\n",
    "    \"\"\"\n",
    "    x.set_shape([None, None, None, 4])  # n, h, w, c\n",
    "    y.set_shape([None, 1])  # n, nb_classes\n",
    "    return x, y\n",
    "\n",
    "g_tiles = 1\n",
    "g_image_size = 512\n",
    "\n",
    "### Open dataframe with files and labels\n",
    "print(\"Reading dataset...\")\n",
    "xr_datasets, extents = build_dataset.load_satellite_datasets()\n",
    "df = build_dataset.load_icpag_dataset()\n",
    "df = build_dataset.assign_links_to_datasets(df, extents, verbose=True)\n",
    "print(\"Dataset loaded!\")\n",
    "\n",
    "print(\"Cleaning dataset...\")\n",
    "# Clean dataframe and create datasets\n",
    "# df = df[df[\"sample\"] <= 2]\n",
    "if small_sample:\n",
    "    df = df.sample(2000, random_state=825).reset_index(drop=True)\n",
    "\n",
    "### Split census tracts based on train/test\n",
    "#       (the hole census tract must be in the corresponding region)\n",
    "df[\"min_x\"] = df.bounds[\"minx\"]\n",
    "df[\"max_x\"] = df.bounds[\"maxx\"]\n",
    "df = build_dataset.split_train_test(df)\n",
    "df = df[[\"link\", \"dataset\", \"var\", \"type\", \"geometry\"]]\n",
    "\n",
    "### Train/Test\n",
    "list_of_datasets = []\n",
    "\n",
    "print()\n",
    "print(\"Benchmarking MSE against the mean\")\n",
    "\n",
    "## Train\n",
    "df_train = df[df[\"type\"] == \"train\"].copy().reset_index(drop=True)\n",
    "assert df_train.shape[0] > 0, f\"Empty train dataset!\"\n",
    "\n",
    "print(f\"Train MSE: {df_train['var'].var()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Generator for the index\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: list(\n",
    "        range(df_train.shape[0])\n",
    "    ),  # The index generator, \n",
    "    tf.uint8,\n",
    ")  # Creates a dataset with all the indexes (0, 1, 2, 3, etc.)\n",
    "\n",
    "train_dataset = train_dataset.shuffle(\n",
    "    buffer_size=50, seed=825, reshuffle_each_iteration=True\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda i: tf.py_function(  # The actual data generator. Passes the index to the function that will process the data.\n",
    "        func=get_data, inp=[i], Tout=[tf.uint8, tf.float32]\n",
    "    ), \n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    ")\n",
    "\n",
    "train_dataset = (\n",
    "    train_dataset.filter(\n",
    "        lambda img, value: (value < 0) | (value >= 0)\n",
    "    )  # Filter out NaN values\n",
    "    .batch(64)  # .map(_fixup_shape)\n",
    "    # .prefetch(tf.data.AUTOTUNE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = np.load(r\"D:\\Maestría\\Tesis\\Repo\\outputs\\test_example_3_imgs.npy\")\n",
    "imgs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = np.load(rf\"D:\\Maestría\\Tesis\\Repo\\outputs\\train_example_2_imgs.npy\")\n",
    "imgs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(18, 18), dpi=300)\n",
    "\n",
    "for pos in range(9):\n",
    "    ax = plt.subplot(3, 3, pos + 1)\n",
    "    img = utils.augment_image(imgs[0])[:,:,:3]\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug[95:105,55:65,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.fliplr(img[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3*1024/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = train_dataset.take(1)\n",
    "lista = list(d.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "import earthpy.plot as ep    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = lista[0][0][0]\n",
    "img = skimage.exposure.equalize_hist(img) # stretch\n",
    "img = np.moveaxis(img, 2, 0)\n",
    "ep.plot_rgb(img,\n",
    "        rgb=[0, 1, 2],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "import earthpy.plot as ep    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = get_data(39)[0]\n",
    "img = skimage.exposure.equalize_hist(img) # stretch\n",
    "img = np.moveaxis(img, 2, 0)\n",
    "ep.plot_rgb(img,\n",
    "        rgb=[0, 1, 2],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "import earthpy.plot as ep    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(2,2, figsize=(16, 16), facecolor='w', edgecolor='k', dpi=200)\n",
    "# fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "axs = axs.ravel()\n",
    "\n",
    "i=0\n",
    "for x in test.take(1):\n",
    "    img = x[0].numpy()[0]\n",
    "    img = skimage.exposure.equalize_hist(img) # stretch\n",
    "    img = np.moveaxis(img, 2, 0)\n",
    "    ep.plot_rgb(img,\n",
    "            rgb=[0, 1, 2],\n",
    "            ax=axs[i])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = list(range(len([2] * 30)))\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5\n",
    "\n",
    "def test1():\n",
    "    a = 3\n",
    "    test2()\n",
    "    \n",
    "def test2():\n",
    "    print(a)\n",
    "    \n",
    "\n",
    "test1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "import earthpy.plot as ep    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(2,2, figsize=(16, 16), facecolor='w', edgecolor='k', dpi=200)\n",
    "# fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "axs = axs.ravel()\n",
    "\n",
    "i=0\n",
    "for x in test.take(4):\n",
    "    img = x[0].numpy()[0]\n",
    "    img = skimage.exposure.equalize_hist(img) # stretch\n",
    "    img = np.moveaxis(img, 2, 0)\n",
    "    ep.plot_rgb(img,\n",
    "            rgb=[0, 1, 2],\n",
    "            ax=axs[i])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=10\n",
    "link = training_set.iloc[i][\"link\"]\n",
    "value = training_set.iloc[i][\"var\"]\n",
    "link_dataset = datasets[icpag.iloc[i][\"dataset\"]]\n",
    "image = utils.random_image_from_census_tract(\n",
    "    link_dataset, training_set, link, tiles=tiles, size=actual_size, bias=bias, image_return_only=True\n",
    ")    \n",
    "image = np.moveaxis(image, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icpag.loc[icpag.link == link, \"dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = icpag\n",
    "link = \"068613306\"\n",
    "tiles = 1\n",
    "actual_size = 128\n",
    "bias = 2\n",
    "\n",
    "\n",
    "link_dataset = datasets[icpag.loc[icpag.link == link, \"dataset\"].values[0]]\n",
    "image = utils.random_image_from_census_tract(\n",
    "    link_dataset, df, link, tiles=tiles, size=actual_size, bias=bias, image_return_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in dataset:\n",
    "    print(x.numpy())\n",
    "    if x==20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=10\n",
    "batch_size=32\n",
    "icpag[index * batch_size:(index + 1) * batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traingen = CustomDataGen(train_df,\n",
    "                         path='filename',\n",
    "                         value='value',\n",
    "                         batch_size=32, input_size=target_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import build_dataset\n",
    "import run_model\n",
    "import utils\n",
    "import matplotlib\n",
    "import geopandas as gpd\n",
    "from shapely import Polygon\n",
    "importlib.reload(build_dataset)\n",
    "importlib.reload(utils)\n",
    "\n",
    "# Load datasets\n",
    "datasets, extents = build_dataset.load_satellite_datasets()\n",
    "icpag = build_dataset.load_icpag_dataset()\n",
    "icpag = build_dataset.assign_links_to_datasets(icpag, extents, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['pansharpened_6741387101_R1C1'].drop_vars(\"spatial_ref\")[['band', 'y', 'x', 'band_data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['pansharpened_6741387101_R1C2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extents['pansharpened_6741387101_R1C1'].boundary.xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(extents['pansharpened_6741387101_R1C1'].boundary.xy[0], extents['pansharpened_6741387101_R1C1'].boundary.xy[1])\n",
    "plt.plot(extents['pansharpened_6741387101_R1C1'].buffer(-0.01).boundary.xy[0], extents['pansharpened_6741387101_R1C1'].buffer(-0.01).boundary.xy[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "poly = extents['pansharpened_6741387101_R1C1'].buffer(0.1)\n",
    "\n",
    "points_list = [(poly.boundary.xy[1][i], poly.boundary.xy[0][i]) for i in range(0, 4)]\n",
    "# Create a map object centered at a specific location\n",
    "m = folium.Map(location=[poly.centroid.y, poly.centroid.x], zoom_start=12)\n",
    "\n",
    "\n",
    "# Create a polygon and add it to the map\n",
    "polygon = folium.Polygon(locations=points_list, color='blue', fill=True, fill_color='red', fill_opacity=0.6)\n",
    "polygon.add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import build_dataset\n",
    "import run_model\n",
    "import utils\n",
    "import matplotlib\n",
    "import geopandas as gpd\n",
    "from shapely import Polygon\n",
    "importlib.reload(build_dataset)\n",
    "importlib.reload(utils)\n",
    "\n",
    "link = '064100610'\n",
    "\n",
    "# Load datasets\n",
    "datasets, extents = build_dataset.load_satellite_datasets()\n",
    "icpag = build_dataset.load_icpag_dataset()\n",
    "icpag = build_dataset.assign_links_to_datasets(icpag, extents, verbose=False)\n",
    "\n",
    "# Get the dataset with the images of the selected link\n",
    "ds = build_dataset.get_dataset_for_link(icpag, datasets, link)\n",
    "\n",
    "# Get the grid of the images\n",
    "images, points, bounds = build_dataset.get_gridded_images_for_link(\n",
    "    ds, icpag, link, tiles=1, size=128, resizing_size=128, bias=4, sample=1, to8bit=True\n",
    ")\n",
    "\n",
    "# Compute loss\n",
    "df_prediciones, mse = run_model.compute_custom_loss(\n",
    "    model, metadata[metadata.link == int(link)], tiles, size, resizing_size, bias, sample, to8bit\n",
    ")\n",
    "\n",
    "# Make geodataframe with the images and its predictions\n",
    "polygons = [Polygon(bound[0]) for bound in bounds]\n",
    "predictions = df_prediciones[df_prediciones['link']==link].predictions.values[0]\n",
    "predictions_gdf =  gpd.GeoDataFrame(predictions, geometry=polygons).rename(columns={0:'predictions'}).set_crs(epsg=4326)\n",
    "\n",
    "#### Plot\n",
    "import folium\n",
    "# Plot census tract\n",
    "m = icpag[icpag.link == link].explore(\n",
    "        tiles=\"https://services.arcgisonline.com/arcgis/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\", attr=\"ESRI\",\n",
    "        \n",
    ")\n",
    "# Plot gridded predictions\n",
    "predictions_gdf.explore(column='predictions', cmap='Spectral', vmin=-1, vmax=1, m=m)\n",
    "\n",
    "# Add control for switching between layers\n",
    "folium.LayerControl().add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import run_model\n",
    "import pandas as pd\n",
    "import importlib\n",
    "importlib.reload(run_model)\n",
    "\n",
    "path_repo = r\"/mnt/d/Maestría/Tesis/Repo/\"\n",
    "modelpath = rf\"{path_repo}/data/data_out/models_by_epoch/mobnet_v3_25\"\n",
    "# model = tf.keras.models.load_model(\n",
    "#            modelpath, compile=True\n",
    "#         )\n",
    "# metadata = pd.read_csv(f\"{path_repo}/data/data_out/train_size128_tiles1_sample10/metadata.csv\")\n",
    "tiles, size, resizing_size, bias, sample = 1, 128, 128, 2, 1\n",
    "\n",
    "df, mse = run_model.compute_custom_loss(\n",
    "    model, metadata, tiles, size, resizing_size, bias, sample, True, verbose=True\n",
    ")a\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.loc[metadata.link ==10 ].icpag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_gdf.explore(column='predictions', cmap='Spectral', vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.concatenate([np.array([10,20,30]),np.array([10])]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorize(prediction, vmin=vmin, vmax=vmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "metadata = pd.read_csv(r\"D:\\Maestría\\Tesis\\Repo\\data\\data_out\\size512_sample5\\metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 20010101\n",
    "metadata.loc[metadata.link == link, 'var'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"./data/icpag.csv\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "path_dataout = r\"D:\\Maestría\\Tesis\\Repo\\data\\data_out\"\n",
    "model_path=fr\"{path_dataout}/models/mobnet_v3_20230831-172738\"\n",
    "\n",
    "keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "img = np.load(r\"D:\\Maestría\\Tesis\\Repo\\data\\data_out\\size128_sample10\\020010103_1.npy\")\n",
    "img = np.moveaxis(img, 0, 2)[:,:,:3]\n",
    "# equalize hist\n",
    "img = skimage.exposure.equalize_hist(img)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import earthpy.plot as ep\n",
    "\n",
    "# ep.plot_rgb(images[0], rgb=[0, 1, 2], title=\"RGB Image\", stretch=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import dotenv_values\n",
    "image_size = 200\n",
    "sample_size = 1\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "env = dotenv_values(\"D:/Maestría/Tesis/Repo/scripts/globals.env\")\n",
    "\n",
    "path_proyecto = env[\"PATH_PROYECTO\"]\n",
    "path_datain = env[\"PATH_DATAIN\"]\n",
    "path_dataout = env[\"PATH_DATAOUT\"]\n",
    "path_scripts = env[\"PATH_SCRIPTS\"]\n",
    "path_satelites = env[\"PATH_SATELITES\"]\n",
    "path_logs = env[\"PATH_LOGS\"]\n",
    "path_outputs = env[\"PATH_OUTPUTS\"]\n",
    "\n",
    "# df = pd.read_csv(\n",
    "#     rf\"D:\\Maestría\\Tesis\\Repo\\data\\data_out\\size500_sample1\\metadata.csv\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import build_dataset\n",
    "importlib.reload(build_dataset)\n",
    "variable = \"ln_pred_inc_mean\"\n",
    "datasets, extents = build_dataset.load_satellite_datasets()\n",
    "icpag = build_dataset.load_icpag_dataset(variable)\n",
    "icpag = build_dataset.assign_links_to_datasets(icpag, extents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import run_model\n",
    "import importlib\n",
    "importlib.reload(run_model)\n",
    "\n",
    "kind=\"reg\"\n",
    "image_size = 500\n",
    "resizing_size = 200\n",
    "my_test = \"ddddddd\"\n",
    "train_dataset, test_dataset, filenames = run_model.create_and_build_datasets(\n",
    "    kind=kind,\n",
    "    image_size=image_size,\n",
    "    resizing_size=resizing_size,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icpag['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "imgs = np.load(r\"D:\\Maestría\\Tesis\\Repo\\outputs\\examples_0_img.npy\")\n",
    "labs = np.load(r\"D:\\Maestría\\Tesis\\Repo\\outputs\\examples_0_lab.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(file_path, label):\n",
    "    import cv2\n",
    "    \n",
    "    img = np.load(file_path)\n",
    "    img = np.moveaxis(\n",
    "        img, 0, 2\n",
    "    )  # Move axis so the original [4, 512, 512] becames [512, 512, 4]\n",
    "    img = cv2.resize(\n",
    "        img, dsize=(resizing_size, resizing_size), interpolation=cv2.INTER_CUBIC\n",
    "    )\n",
    "\n",
    "    img = tf.convert_to_tensor(img / 255, dtype=tf.float32)\n",
    "    label = tf.cast(label, tf.float32)\n",
    "\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "resizing_size = 200\n",
    "img, label = process_image(r\"D:\\Maestría\\Tesis\\Repo\\data\\data_out\\size500_sample1\\062742004_0.npy\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "example = tfds.as_numpy(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(example[:,:,0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = r\"D:\\Maestría\\Tesis\\Repo\\data\\data_out\\size500_sample1\"\n",
    "files = [f\"{path}\\{file}\" for file in os.listdir(path)]\n",
    "\n",
    "labels = [0] * len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((files, labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "    lambda file, label: tf.numpy_function(\n",
    "        process_image, [file, label], (tf.float32, tf.float32)\n",
    "    )\n",
    ")  # Parse every image in the dataset using `map`\n",
    "\n",
    "imgs = []\n",
    "labs = []\n",
    "for x in dataset.take(1):\n",
    "    imgs += [tfds.as_numpy(x)[0]]\n",
    "    labs += [tfds.as_numpy(x)[1]]\n",
    "    \n",
    "imgs\n",
    "#### HASTA ACA TODO GENIAL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "        [\n",
    "            layers.RandomFlip(\n",
    "                \"horizontal_and_vertical\",\n",
    "                seed=825,\n",
    "                input_shape=(resizing_size, resizing_size, 4),\n",
    "            ),\n",
    "            # layers.RandomTranslation(0.3, 0.3, fill_mode=\"reflect\", seed=825),\n",
    "            # layers.RandomHeight(0.3),\n",
    "            # layers.RandomWidth(0.3),\n",
    "            # layers.RandomZoom(0.3, seed=825),\n",
    "            # layers.RandomContrast(0.3, seed=825),\n",
    "            # layers.RandomBrightness(0.05, value_range=(0,1), seed=825),\n",
    "            # layers.RandomCrop(image_size, image_size, seed=825),\n",
    "        ],\n",
    "        name=\"data_augmentation\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    dataset.shuffle(round(len(files[0]) / 10))\n",
    "    .batch(32)\n",
    "    .map(lambda x, y: (data_augmentation(x), y))\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "ds = datasets[icpag.loc[icpag.link == \"065150214\", \"dataset\"].values[0]]\n",
    "composition, point, boundaries, total_boundaries = utils.random_image_from_census_tract(\n",
    "    ds, icpag, \"065150214\", tiles=2, size=500, bias=4, to8bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "labs = []\n",
    "for x in train_dataset.take(1):\n",
    "    imgs += [tfds.as_numpy(x)[0]]\n",
    "    labs += [tfds.as_numpy(x)[1]]\n",
    "\n",
    "print(imgs[0][0].shape)    \n",
    "plt.imshow(imgs[0][0][:,:,:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1\n",
    "image_size = 500\n",
    "path_dataout = r\"D:/Maestría/Tesis/Repo/data/data_out\"\n",
    "\n",
    "df = pd.read_csv(\n",
    "    rf\"{path_dataout}/size{image_size}_sample{sample_size}/metadata.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_column_to_x_y(df):\n",
    "    df.point = df.point.str.replace(\"\\(|\\)\", \"\", regex=True).str.split(\",\")\n",
    "    df['x'] = df.point.str[0]\n",
    "    df['y']= df.point.str[1]\n",
    "    return df[['x', 'y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "df = df.dropna(how=\"any\").reset_index(drop=True)\n",
    "df[['x', 'y']] = point_column_to_x_y(df)\n",
    "df.x, df.y = df.x.astype(float), df.y.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import build_dataset\n",
    "import pandas as pd\n",
    "path_dataout = r\"D:/Maestría/Tesis/Repo/data/data_out\"\n",
    "\n",
    "# build_dataset.build_dataset(200, 1, variable=\"ln_pred_inc_mean\")\n",
    "df = pd.read_csv(\n",
    "    rf\"{path_dataout}/size200_sample1/metadata.csv\"\n",
    ")\n",
    "\n",
    "metadata = build_dataset.split_train_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import build_dataset\n",
    "import utils\n",
    "importlib.reload(build_dataset)\n",
    "importlib.reload(utils)\n",
    "importlib.reload(build_dataset)\n",
    "build_dataset.build_dataset(500, 1, tiles=2, bias=4, variable=\"ln_pred_inc_mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(build_dataset)\n",
    "df = pd.read_csv(\n",
    "    rf\"{path_dataout}/size500_sample1/metadata.csv\"\n",
    ")\n",
    "# df = df.drop(columns=\"Test\")\n",
    "# df = build_dataset.split_train_test(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
